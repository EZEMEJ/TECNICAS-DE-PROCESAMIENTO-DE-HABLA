{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mzBx4CUBZLe1",
        "Mx_hzi_2ZeLf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TRABAJO PR√ÅCTICO FINAL INTEGRADOR\n",
        "\n",
        "Este proyecto es un notebook de Google Colab que permite realizar web scraping de art√≠culos de noticias de Infobae y La Naci√≥n, unificarlos en un archivo CSV y aplicar diversas t√©cnicas de procesamiento de lenguaje natural (NLP) y an√°lisis de datos. Finalmente, se crea una interfaz interactiva utilizando Gradio para visualizar y analizar el contenido extra√≠do y procesado.\n",
        "\n",
        "## Caracter√≠sticas:\n",
        "\n",
        "- **Web Scraping:** Extrae t√≠tulos y texto de los √∫ltimos art√≠culos de Infobae y La Naci√≥n.\n",
        "- **Unificaci√≥n de Datos:** Combina los datos de ambos sitios en un √∫nico DataFrame y lo exporta a un archivo CSV.\n",
        "- **Procesamiento de Lenguaje Natural (NLP):**\n",
        "    - **Limpieza y Tokenizaci√≥n:** Preprocesamiento del texto para eliminar ruido y dividirlo en unidades significativas.\n",
        "    - **Generaci√≥n de Nubes de Palabras (WordCloud):** Visualizaci√≥n de las palabras m√°s frecuentes en el texto.\n",
        "    - **Extracci√≥n de Entidades Nombradas (NER):** Identificaci√≥n y clasificaci√≥n de personas, lugares y organizaciones mencionadas en los art√≠culos utilizando Gemini.\n",
        "    - **An√°lisis de Sentimiento:** Determinaci√≥n de la polaridad (positivo, negativo, neutro) del texto utilizando TextBlob y visualizaci√≥n de la distribuci√≥n general de sentimientos.\n",
        "    - **Generaci√≥n de Res√∫menes y Tweets:** Creaci√≥n de res√∫menes concisos y tweets con modismos argentinos utilizando Gemini.\n",
        "- **Interfaz Interactiva con Gradio:** Permite seleccionar art√≠culos, visualizar su texto, aplicar funciones de NLP y ver los resultados de forma interactiva.\n",
        "\n",
        "## C√≥mo usar:\n",
        "\n",
        "1. **Abrir el Notebook:** Abre el notebook en Google Colab.\n",
        "2. **Ejecutar las Celdas:** Ejecuta secuencialmente todas las celdas del notebook. Esto instalar√° las dependencias, realizar√° el web scraping, procesar√° los datos y lanzar√° la interfaz Gradio.\n",
        "3. **Interactuar con la Interfaz Gradio:** Una vez que la interfaz Gradio se haya lanzado (aparecer√° un enlace p√∫blico), √°brela en tu navegador.\n",
        "    - Selecciona un art√≠culo del men√∫ desplegable.\n",
        "    - Utiliza los botones para:\n",
        "        - \"Mostrar texto\": Ver el texto completo del art√≠culo.\n",
        "        - \"Limpiar texto\": Ver el texto despu√©s de la limpieza y tokenizaci√≥n.\n",
        "        - \"Generar WordCloud\": Visualizar la nube de palabras.\n",
        "        - \"Extraer Entidades (NER)\": Ver las entidades nombradas extra√≠das.\n",
        "        - \"An√°lisis de Sentimiento\": Ver el sentimiento del conjunto de art√≠culos y la distribuci√≥n general de sentimientos.\n",
        "        - \"Resumen\": Obtener un resumen del art√≠culo generado por Gemini.\n",
        "        - \"Generador de Tweet\": Obtener un tweet generado por Gemini basado en el art√≠culo.\n",
        "\n",
        "## Requisitos:\n",
        "\n",
        "- Cuenta de Google para acceder a Google Colab.\n",
        "- Clave de API de Google Gemini (configurada en los secretos de Colab como `GOOGLE_API_KEY`).\n",
        "\n",
        "## Dependencias:\n",
        "\n",
        "Las dependencias se instalan autom√°ticamente al ejecutar la primera celda del notebook (`!pip install...`). Incluyen:\n",
        "\n",
        "- `gradio`\n",
        "- `transformers`\n",
        "- `sentencepiece`\n",
        "- `spacy`\n",
        "- `wordcloud`\n",
        "- `matplotlib`\n",
        "- `textblob`\n",
        "- `requests`\n",
        "- `beautifulsoup4`\n",
        "- `pandas`\n",
        "- `google-generativeai`\n",
        "- `nltk`\n",
        "\n",
        "## Archivos generados:\n",
        "\n",
        "- `datos_combinados.csv`: Archivo CSV que contiene los art√≠culos scrapeados de Infobae y La Naci√≥n."
      ],
      "metadata": {
        "id": "7T4QK3LoZEJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Instalaci√≥n de librer√≠as y m√≥dulos necesarios."
      ],
      "metadata": {
        "id": "mzBx4CUBZLe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gradio transformers sentencepiece spacy wordcloud matplotlib textblob\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "JMOzAW_ScWRV",
        "collapsed": true
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from random import uniform\n",
        "from google.colab import userdata\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import spacy\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from textblob import TextBlob\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead, pipeline\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "DMXn6_uQTUYf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Web Scraping de Infobae y La Naci√≥n"
      ],
      "metadata": {
        "id": "Mx_hzi_2ZeLf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dhc2SYVE8wqt"
      },
      "outputs": [],
      "source": [
        "## WEB SCRAPING INFOBAE ##\n",
        "\n",
        "# URL base\n",
        "BASE_URL = \"https://www.infobae.com\"\n",
        "\n",
        "# URL que contiene los links de art√≠culos (puedes cambiar esta a otra categor√≠a o portada)\n",
        "page_url = \"https://www.infobae.com/ultimas-noticias/\"\n",
        "\n",
        "# Obtener el HTML\n",
        "response = requests.get(page_url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Extraer todos los <a> con clase \"story-card-ctn\"\n",
        "links = soup.find_all(\"a\", class_=\"feed-list-card\")\n",
        "\n",
        "# Extraer href y guardar en lista\n",
        "hrefs = [link.get(\"href\") for link in links if link.get(\"href\")]\n",
        "\n",
        "# Limitar a los primeros 20\n",
        "hrefs = hrefs[:20]\n",
        "\n",
        "# Para almacenar los resultados\n",
        "data = []\n",
        "\n",
        "# Recorrer cada href\n",
        "for href in hrefs:\n",
        "    full_url = BASE_URL + href if href.startswith(\"/\") else href\n",
        "    try:\n",
        "        # Peque√±a pausa para no sobrecargar el servidor\n",
        "        sleep(uniform(1, 3))\n",
        "\n",
        "        article_response = requests.get(full_url)\n",
        "        article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
        "\n",
        "        # Extraer el t√≠tulo del art√≠culo\n",
        "        h1_element = article_soup.find('h1')\n",
        "        id_value = h1_element.get('id', '')\n",
        "        titulo = id_value.replace('-', ' ')\n",
        "\n",
        "        # Extraer todos los <p> y juntar su texto\n",
        "        paragraphs = article_soup.find_all(\"p\")\n",
        "        texto = \" \".join([p.get_text(strip=False) for p in paragraphs])\n",
        "\n",
        "        # Guardar en data\n",
        "        data.append([\"infobae\", titulo, texto])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {full_url}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Crear DataFrame\n",
        "df_infobae = pd.DataFrame(data, columns=[\"fuente\", \"titulo\", \"texto\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- WEB SCRAPING LA NACION ---\n",
        "\n",
        "# URL base\n",
        "BASE_URL = \"https://www.lanacion.com.ar\"\n",
        "page_url = \"https://www.lanacion.com.ar/ultimas-noticias/\"\n",
        "\n",
        "# Obtener HTML de la p√°gina principal\n",
        "response = requests.get(page_url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Extraer todos los <a> con href (sin clase espec√≠fica)\n",
        "links = soup.select(\"h2 a[href]\")\n",
        "\n",
        "# Filtrar hrefs que parezcan links v√°lidos de art√≠culos (opcional: que contengan alguna palabra clave o estructura)\n",
        "hrefs = [link['href'] for link in links if link['href'].startswith('/')]\n",
        "\n",
        "# Evitar duplicados\n",
        "hrefs = list(set(hrefs))\n",
        "\n",
        "# Limitar a los primeros 20\n",
        "hrefs = hrefs[:20]\n",
        "\n",
        "# Lista para almacenar los datos\n",
        "data = []\n",
        "\n",
        "# Recorrer los hrefs y scrapear cada art√≠culo\n",
        "for href in hrefs:\n",
        "    full_url = BASE_URL + href\n",
        "    try:\n",
        "        sleep(uniform(1, 3))  # Pausa para evitar bloqueo\n",
        "\n",
        "        article_response = requests.get(full_url)\n",
        "        article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
        "\n",
        "        # Extraer el t√≠tulo del art√≠culo\n",
        "        h1_element = article_soup.find('h1', class_=\"com-title\")\n",
        "        titulo = \"\".join(h1_element.get_text(strip=False))\n",
        "\n",
        "        # Extraer todo el texto de los <p>\n",
        "        paragraphs = article_soup.find_all(\"p\", class_=\"com-paragraph\")\n",
        "        texto = \" \".join([p.get_text(strip=False) for p in paragraphs])\n",
        "\n",
        "        # Agregar a la lista de resultados\n",
        "        data.append([\"lanacion\", titulo, texto])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar {full_url}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Crear un DataFrame con los resultados\n",
        "df_lanacion = pd.DataFrame(data, columns=[\"fuente\", \"titulo\", \"texto\"])\n",
        "\n",
        "df_lanacion.to_csv(\"lanacion.csv\", index=False)"
      ],
      "metadata": {
        "id": "IQkqmL2fBILP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unificamos los df de infobae y la nacion en un unico df que exportamos\n",
        "\n",
        "\n",
        "df = pd.concat([df_infobae, df_lanacion], ignore_index=True)\n",
        "\n",
        "# Guardar en un CSV\n",
        "df.to_csv(\"datos_combinados.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ CSV guardado como 'datos_combinados.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtAf2setFk36",
        "outputId": "770b54c4-db8a-4efd-fa0f-ce36529260de"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ CSV guardado como 'datos_combinados.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Funciones"
      ],
      "metadata": {
        "id": "6JnlA2jXZo55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo de Gemini\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "cliente = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "MODEL_ID = \"gemini-2.5-flash-preview-05-20\"\n",
        "\n",
        "# Cargar modelo spaCy espa√±ol\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "nltk.download('punkt')  # Para la tokenizaci√≥n de palabras\n",
        "nltk.download('stopwords')  # Para las palabras comunes\n",
        "\n",
        "# Cargar CSV\n",
        "df = pd.read_csv(\"datos_combinados.csv\")\n",
        "\n",
        "# Opciones para dropdown\n",
        "opciones = df[\"titulo\"].dropna().astype(str).unique().tolist()\n",
        "\n",
        "# Cargar el modelo de resumen de Hugging Face (T5 o BART para espa√±ol)\n",
        "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "# summarizer = pipeline(\"summarization\", model=\"josmunpen/mt5-small-spanish-summarization\")\n",
        "# summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\")\n",
        "\n",
        "# Funci√≥n para mostrar texto\n",
        "def mostrar_texto(href_seleccionado):\n",
        "    fila = df[df[\"titulo\"].astype(str) == str(href_seleccionado)]\n",
        "    if fila.empty:\n",
        "        return \"‚ö†Ô∏è No se encontr√≥ el art√≠culo.\"\n",
        "    texto = fila.iloc[0][\"texto\"]\n",
        "    if not texto or texto.strip() == \"\":\n",
        "        return \"‚ö†Ô∏è Art√≠culo sin contenido v√°lido.\"\n",
        "    return texto\n",
        "\n",
        "# 1. Funciones de limpieza y wordcloud\n",
        "\n",
        "# Funci√≥n de Limpieza\n",
        "def limpiar_texto(texto):\n",
        "    texto = texto[11:]\n",
        "    texto = re.sub(r\"[^\\w\\s]\", \"\", texto)  # Quitar puntuaci√≥n\n",
        "    texto = re.sub(r\"\\d+\", \"\", texto)      # Quitar n√∫meros\n",
        "    texto = texto.lower()       # Normalizar y pasar a min√∫sculas\n",
        "    return texto.strip()\n",
        "\n",
        "\n",
        "# Preprocesamiento NLP\n",
        "def procesar_texto(texto):\n",
        "    texto_limpio = limpiar_texto(texto)\n",
        "    doc = nlp(texto_limpio)\n",
        "    tokens = [t.lemma_ for t in doc if t.is_alpha and not t.is_stop]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "# Generar WordCloud\n",
        "def generar_wordcloud(texto):\n",
        "    if not texto or not isinstance(texto, str) or texto.strip() == \"\":\n",
        "        print(\"‚ö†Ô∏è Texto vac√≠o o inv√°lido.\")\n",
        "        return None\n",
        "    try:\n",
        "        doc = nlp(texto)\n",
        "        tokens = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop and len(token.text) > 2]\n",
        "        if not tokens:\n",
        "            print(\"‚ö†Ô∏è No hay tokens v√°lidos despu√©s de filtrar.\")\n",
        "            return None\n",
        "        texto_procesado = \" \".join(tokens)\n",
        "        wordcloud = WordCloud(width=800, height=400, max_words=100, background_color=\"white\").generate(texto_procesado)\n",
        "        buf = BytesIO()\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.tight_layout(pad=0)\n",
        "        plt.savefig(buf, format='png')\n",
        "        plt.close()\n",
        "        buf.seek(0)\n",
        "        img = Image.open(buf)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generando WordCloud: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# 2. Funci√≥n para NER: extraer y contar entidades en texto seleccionado\n",
        "\n",
        "def realizar_ner(texto):\n",
        "    pregunta = f\"\"\"\n",
        "    Extra√© todas las entidades nombradas del siguiente texto en espa√±ol argentino y clasificalas, solo necesito la respuesta:\n",
        "\n",
        "    CATEGOR√çAS:\n",
        "    - PERSONA: Nombres de personas\n",
        "    - LUGAR: Ciudades, pa√≠ses, barrios, direcciones, lugares espec√≠ficos\n",
        "    - ORGANIZACI√ìN: Empresas, universidades, instituciones\n",
        "\n",
        "    FORMATO DE RESPUESTA:\n",
        "    [ENTIDAD] ‚Üí [CATEGOR√çA] ‚Üí [BREVE EXPLICACI√ìN]\n",
        "\n",
        "    TEXTO A ANALIZAR:\n",
        "    {texto}\n",
        "    \"\"\"\n",
        "    respuesta = cliente.models.generate_content(\n",
        "                model=MODEL_ID,\n",
        "                contents=[pregunta] # Pasa la pregunta como contenido\n",
        "                    )\n",
        "    return respuesta.text\n",
        "\n",
        "\n",
        "\n",
        "# 3. Funciones para an√°lisis de sentimiento\n",
        "\n",
        "# Funci√≥n auxiliar para clasificar polaridad con TextBlob\n",
        "def clasificar_sentimiento(texto):\n",
        "    if not texto or texto.strip() == \"\":\n",
        "        return \"Neutro\"\n",
        "    try:\n",
        "        blob = TextBlob(texto)\n",
        "        pol = blob.sentiment.polarity\n",
        "        if pol > 0.1:\n",
        "            return \"Positivo\"\n",
        "        elif pol < -0.1:\n",
        "            return \"Negativo\"\n",
        "        else:\n",
        "            return \"Neutro\"\n",
        "    except:\n",
        "        return \"Neutro\"\n",
        "\n",
        "# Calcular sentimiento para todas las noticias del csv\n",
        "df['sentimiento'] = df['texto'].fillna(\"\").apply(clasificar_sentimiento)\n",
        "\n",
        "# Gr√°fico de barras con distribuci√≥n general\n",
        "def graficar_distribucion_sentimientos():\n",
        "    conteo = df['sentimiento'].value_counts()\n",
        "    plt.figure(figsize=(6,4))\n",
        "    conteo.plot(kind='bar', color=['green','red','gray'])\n",
        "    plt.title(\"Distribuci√≥n de sentimiento en art√≠culos\")\n",
        "    plt.ylabel(\"Cantidad de art√≠culos\")\n",
        "    plt.xlabel(\"Sentimiento\")\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close()\n",
        "    buf.seek(0)\n",
        "    return Image.open(buf)\n",
        "\n",
        "# Mostrar sentimiento y gr√°fica de distribuci√≥n al seleccionar art√≠culo\n",
        "def analizar_sentimiento(titulo_articulo):\n",
        "    fila = df[df[\"titulo\"].astype(str) == str(titulo_articulo)]\n",
        "    if fila.empty:\n",
        "        return \"No se encontr√≥ el art√≠culo.\", None\n",
        "    texto = fila.iloc[0][\"texto\"]\n",
        "    sentimiento_articulo = clasificar_sentimiento(texto)  # Analiza solo este texto\n",
        "    grafico = graficar_distribucion_sentimientos()\n",
        "    texto_salida = f\"Sentimiento del art√≠culo seleccionado: **{sentimiento_articulo}**\"\n",
        "    return texto_salida, grafico\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 4. Funci√≥n de resumen y generaci√≥n de tweet\n",
        "\n",
        "# Funci√≥n para generar resumen\n",
        "def generar_resumen(texto):\n",
        "    pregunta = f\"\"\"Sumariza el siguiente texto en tres oraciones seguidas relacionadas de rapida lectura\n",
        "              Texto: {texto}\n",
        "              \"\"\"\n",
        "\n",
        "    resumen_final = cliente.models.generate_content(\n",
        "                    model=MODEL_ID,\n",
        "                    contents=[pregunta] # Pasa la pregunta como contenido\n",
        "                    )\n",
        "    return resumen_final.text\n",
        "\n",
        "\n",
        "# Funci√≥n para generar tweet\n",
        "def generar_tweet(texto):\n",
        "    pregunta = f\"\"\"Sumariza en un tweet de 80 palabras m√°ximo utilizando modismos argentinos, no es necesario que remarques los modismos\n",
        "              Texto: {texto}\n",
        "              \"\"\"\n",
        "\n",
        "    tweet_final = cliente.models.generate_content(\n",
        "                    model=MODEL_ID,\n",
        "                    contents=[pregunta] # Pasa la pregunta como contenido\n",
        "                    )\n",
        "    return tweet_final.text\n",
        "\n"
      ],
      "metadata": {
        "id": "YKelagR7SerF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f66ae7-2615-4cc3-9b0a-9f002af8359e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Interfaz Gradio"
      ],
      "metadata": {
        "id": "VAHeT_DDZ3CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- INTERFAZ GRADIO ---\n",
        "\n",
        "with gr.Blocks() as interfaz:\n",
        "    gr.Markdown(\"## üì∞ Visualizador\")\n",
        "\n",
        "    dropdown = gr.Dropdown(choices=opciones, label=\"Seleccion√° un art√≠culo\")\n",
        "\n",
        "    with gr.Row():\n",
        "        boton_texto = gr.Button(\"Mostrar texto\")\n",
        "        boton_limpieza = gr.Button(\"Limpiar texto\")\n",
        "        boton_wc = gr.Button(\"Generar WordCloud\")\n",
        "        boton_ner = gr.Button(\"Extraer Entidades (NER)\")\n",
        "        boton_sent = gr.Button(\"An√°lisis de Sentimiento\")\n",
        "\n",
        "    with gr.Row():\n",
        "        boton_resumen = gr.Button(\"Resumen\")\n",
        "        boton_opinion = gr.Button(\"Generador de Tweet\")\n",
        "\n",
        "    salida = gr.Textbox(label=\"Texto del art√≠culo\", lines=15)\n",
        "    salida_texto = gr.Textbox(label=\"Limpieza\", lines=15)\n",
        "    salida_imagen = gr.Image(type=\"pil\", label=\"WordCloud\")\n",
        "    salida_ner = gr.Textbox(label=\"Entidades nombradas extra√≠das\")\n",
        "    salida_sentimiento = gr.Markdown(label=\"Resultado de an√°lisis de sentimiento\")\n",
        "    salida_grafico_sent = gr.Image(type=\"pil\", label=\"Distribuci√≥n general sentimientos del conjunto de noticias\")\n",
        "    salida_resumen = gr.Textbox(label=\"Resumen de la noticia\", lines=3)\n",
        "    salida_opinion = gr.Textbox(label=\"Generador de Tweet\", lines=4)\n",
        "\n",
        "\n",
        "    # Eventos botones\n",
        "    boton_texto.click(fn=mostrar_texto, inputs=dropdown, outputs=salida)\n",
        "    boton_limpieza.click(fn=procesar_texto, inputs=salida, outputs=salida_texto)\n",
        "    boton_wc.click(fn=generar_wordcloud, inputs=salida_texto, outputs=salida_imagen)\n",
        "    boton_ner.click(fn=realizar_ner, inputs=salida_texto, outputs=salida_ner)\n",
        "    boton_sent.click(fn=analizar_sentimiento, inputs=dropdown, outputs=[salida_sentimiento, salida_grafico_sent])\n",
        "    boton_resumen.click(fn=generar_resumen, inputs=salida_texto, outputs=salida_resumen)\n",
        "    boton_opinion.click(fn=generar_tweet, inputs=salida_texto, outputs=salida_opinion)\n",
        "\n",
        "\n",
        "interfaz.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "MFmvsG0RXI5e",
        "outputId": "5b299138-c49f-4bd6-df13-5782748a5838"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://988e7dab409dbcefdd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://988e7dab409dbcefdd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}