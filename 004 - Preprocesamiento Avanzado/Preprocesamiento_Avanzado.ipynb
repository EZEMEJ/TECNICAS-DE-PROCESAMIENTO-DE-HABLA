{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cuaderno Clase PLN - Preprocesamiento Avanzado\n",
        "\n",
        "**Objetivos:**\n",
        "*   Repasar la limpieza básica de texto.\n",
        "*   Entender y aplicar Stemming (NLTK).\n",
        "*   Entender y aplicar Lematización (spaCy).\n",
        "*   Comparar los resultados de ambas técnicas.\n",
        "*   Reflexionar sobre el impacto del preprocesamiento.\n",
        "\n",
        "**Agenda:**\n",
        "\n",
        "1.  Instalaciones e Importaciones\n",
        "2.  Repaso: Limpieza básica y Tokenización\n",
        "3.  El problema de las variantes de palabras\n",
        "4.  Stemming con NLTK\n",
        "5.  Lematización con spaCy\n",
        "6.  Comparación Stemming vs. Lematización\n",
        "7.  Micro-Laboratorio (Ejercicio Práctico)\n",
        "8.  Brainstorming"
      ],
      "metadata": {
        "id": "8_NHYb-fQu3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Instalaciones e Importaciones"
      ],
      "metadata": {
        "id": "iu4TosgXReRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar librerías (si no están ya en Colab)\n",
        "!pip install nltk spacy > /dev/null\n",
        "!python -m spacy download es_core_news_sm > /dev/null # Modelo pequeño de español para spaCy"
      ],
      "metadata": {
        "id": "2CyxzGvIRdxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías\n",
        "import nltk\n",
        "import spacy\n",
        "import re # Para expresiones regulares (limpieza)\n",
        "import string # Para signos de puntuación"
      ],
      "metadata": {
        "id": "YNOnXGQsRrII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "176_JS8VR2FR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6dd70fe-e579-4457-f136-6e79fa2af3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar modelo de spaCy en español\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "print(\"Modelo de spaCy 'es_core_news_sm' cargado.\")"
      ],
      "metadata": {
        "id": "2oqkr1o2SCab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168e7988-a063-415b-da40-ba3f68f2862e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo de spaCy 'es_core_news_sm' cargado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar stopwords en español de NLTK\n",
        "stopwords_es = nltk.corpus.stopwords.words('spanish')"
      ],
      "metadata": {
        "id": "RHvRsNlySKbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Añadir algunas stopwords comunes si es necesario (opcional)\n",
        "#stopwords_es.extend(['tan', 'van', 'ser', 'haber', 'ir'])\n",
        "print(f\"\\nEjemplo de stopwords en español (NLTK): {stopwords_es[:15]}...\")"
      ],
      "metadata": {
        "id": "1Yq3GXkdSNqh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc2fdf4-8e28-4cb4-95ab-8c06a1495516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ejemplo de stopwords en español (NLTK): ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Repaso: Limpieza básica y Tokenización\n",
        "\n",
        "Recordemos los pasos comunes que ya vimos:\n",
        "*   **Pasar a minúsculas:** Para tratar \"Hola\" y \"hola\" igual.\n",
        "*   **Quitar números:** A menudo no aportan significado general.\n",
        "*   **Quitar signos de puntuación:** Como ',', '.', '!', '?'.\n",
        "*   **Quitar stopwords:** Palabras muy comunes (\"el\", \"la\", \"de\", \"que\", \"y\"...) que aparecen mucho pero no suelen distinguir el tema del texto.\n",
        "*   **Tokenización:** Dividir el texto en unidades (palabras o \"tokens\")."
      ],
      "metadata": {
        "id": "2P1d3ILUlAC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de texto\n",
        "texto_ejemplo = \"Los niños corrían rápidamente por el parque, jugando y riendo. ¡Qué día más lindo!\""
      ],
      "metadata": {
        "id": "MbQyV_bJlKHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función simple de limpieza y tokenización (usando NLTK para stopwords y tokenización)\n",
        "def limpiar_tokenizar_basico(texto):\n",
        "  # 1. Minúsculas\n",
        "  texto = texto.lower()\n",
        "  # 2. Quitar números (usando expresiones regulares)\n",
        "  texto = re.sub(r'\\d+', '', texto)\n",
        "  # 3. Quitar puntuación\n",
        "  texto = texto.translate(str.maketrans('', '', string.punctuation + '¡¿'))\n",
        "  # 4. Quitar espacios extra\n",
        "  texto = texto.strip()\n",
        "  # 5. Tokenizar\n",
        "  tokens = nltk.word_tokenize(texto, language='spanish')\n",
        "  # 6. Quitar stopwords\n",
        "  tokens_limpios = [palabra for palabra in tokens if palabra not in stopwords_es]\n",
        "  return tokens_limpios"
      ],
      "metadata": {
        "id": "DONcWdH2lNpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar la función\n",
        "tokens_basicos = limpiar_tokenizar_basico(texto_ejemplo)\n",
        "print(\"Texto original:\", texto_ejemplo)\n",
        "print(\"Tokens después de limpieza básica y quitar stopwords (NLTK):\", tokens_basicos)"
      ],
      "metadata": {
        "id": "DV-6TnQMlxTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468bac4f-602b-491a-ad90-04897ca66ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Los niños corrían rápidamente por el parque, jugando y riendo. ¡Qué día más lindo!\n",
            "Tokens después de limpieza básica y quitar stopwords (NLTK): ['niños', 'corrían', 'rápidamente', 'parque', 'jugando', 'riendo', 'día', 'lindo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. El problema de las variantes de palabras\n",
        "\n",
        "Observen los tokens resultantes: `['niños', 'corrían', 'rápidamente', 'parque', 'jugando', 'riendo', 'día', 'lindo']`.\n",
        "\n",
        "Tenemos \"corrían\", \"jugando\", \"riendo\". Si tuviéramos otro texto con \"correr\", \"juega\", \"reír\", serían tokens diferentes.\n",
        "\n",
        "**¿No sería útil agrupar las palabras que comparten una raíz o significado base?**\n",
        "\n",
        "*   **corrían, correr, corremos, corrió -> CORRER**\n",
        "*   **jugando, juega, jugamos -> JUGAR**\n",
        "\n",
        "Esto ayuda a:\n",
        "*   Reducir el tamaño del vocabulario (menos columnas en BoW/TF-IDF).\n",
        "*   Agrupar conceptos similares.\n",
        "\n",
        "Dos técnicas principales para esto: **Stemming** y **Lematización**."
      ],
      "metadata": {
        "id": "NcrJG67umD0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Stemming con NLTK\n",
        "\n",
        "*   **¿Qué es?** Un proceso **heurístico** (basado en reglas simples) para cortar el final de las palabras y obtener su \"raíz\" o \"stem\".\n",
        "*   **No siempre produce una palabra real** del diccionario.\n",
        "*   **Ventajas:** Rápido, simple, reduce mucho el vocabulario.\n",
        "*   **Desventajas:** A veces \"corta\" demasiado o agrupa palabras incorrectamente. No considera el contexto gramatical.\n",
        "*   **Herramienta:** NLTK tiene `SnowballStemmer` para español."
      ],
      "metadata": {
        "id": "y1n0lY4-mQLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Crear un stemmer para español\n",
        "stemmer = SnowballStemmer('spanish')"
      ],
      "metadata": {
        "id": "WCrfk44pmEUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a aplicar stemming a los tokens que obtuvimos antes (después de limpieza básica)\n",
        "stems_nltk = [stemmer.stem(token) for token in tokens_basicos]\n",
        "\n",
        "print(\"Tokens originales (limpios):\", tokens_basicos)\n",
        "print(\"Stems resultantes (NLTK): \", stems_nltk)"
      ],
      "metadata": {
        "id": "3P_DAqeImfYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4e8197-e3e7-4f74-a77b-6478f0e6ab3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens originales (limpios): ['niños', 'corrían', 'rápidamente', 'parque', 'jugando', 'riendo', 'día', 'lindo']\n",
            "Stems resultantes (NLTK):  ['niñ', 'corr', 'rapid', 'parqu', 'jug', 'riend', 'dia', 'lind']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Probemos con otras palabras relacionadas\n",
        "palabras_relacionadas = ['correr', 'corría', 'corriendo', 'corredor', 'corredores']\n",
        "stems_relacionadas = [stemmer.stem(p) for p in palabras_relacionadas]\n",
        "print(f\"\\nStemming para {palabras_relacionadas}: {stems_relacionadas}\") # Notar que agrupa bien"
      ],
      "metadata": {
        "id": "MLFpz2X_mbJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8d9344-689a-4b3d-d6e9-fc536bb56f1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemming para ['correr', 'corría', 'corriendo', 'corredor', 'corredores']: ['corr', 'corr', 'corr', 'corredor', 'corredor']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "palabras_problematicas = ['computadora', 'computación', 'computar']\n",
        "stems_problematicos = [stemmer.stem(p) for p in palabras_problematicas]\n",
        "print(f\"Stemming para {palabras_problematicas}: {stems_problematicos}\") # Funciona razonable"
      ],
      "metadata": {
        "id": "HopByEBZmlkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f828960-fc63-414e-c988-ed5694e42e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming para ['computadora', 'computación', 'computar']: ['comput', 'comput', 'comput']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "palabras_problematicas_2 = ['universidad', 'universo']\n",
        "stems_problematicos_2 = [stemmer.stem(p) for p in palabras_problematicas_2]\n",
        "print(f\"Stemming para {palabras_problematicas_2}: {stems_problematicos_2}\") # ¡Ojo! Puede agrupar de más"
      ],
      "metadata": {
        "id": "oyNFRHInmm7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10aae7a8-d63c-4adc-90b2-8e9da3335f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming para ['universidad', 'universo']: ['univers', 'univers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Lematización con spaCy\n",
        "\n",
        "*   **¿Qué es?** Un proceso más **lingüístico**, basado en diccionarios y análisis morfológico, para encontrar la forma canónica o de diccionario de una palabra (su \"lema\").\n",
        "*   **Produce palabras reales**.\n",
        "*   **Ventajas:** Más preciso conceptualmente, mejor para análisis semántico.\n",
        "*   **Desventajas:** Más lento computacionalmente, requiere modelos lingüísticos (como los de spaCy).\n",
        "*   **Herramienta:** spaCy lo hace automáticamente al procesar el texto con un modelo cargado (`nlp()`). El lema está en el atributo `token.lemma_`. spaCy también identifica stopwords (`token.is_stop`)."
      ],
      "metadata": {
        "id": "o0JXR2rRnIa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usaremos spaCy directamente sobre el texto original limpio (sin quitar stopwords aún)\n",
        "# porque spaCy necesita el contexto para lematizar bien.\n",
        "\n",
        "def limpiar_texto_spacy(texto):\n",
        "  # 1. Minúsculas\n",
        "  texto = texto.lower()\n",
        "  # 2. Quitar números\n",
        "  texto = re.sub(r'\\d+', '', texto)\n",
        "  # 3. Quitar puntuación (dejamos espacios)\n",
        "  texto = texto.translate(str.maketrans(string.punctuation + '¡¿', ' ' * len(string.punctuation + '¡¿')))\n",
        "  # 4. Quitar espacios extra\n",
        "  texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "  return texto"
      ],
      "metadata": {
        "id": "1RVojUlInWZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_limpio_spacy = limpiar_texto_spacy(texto_ejemplo)\n",
        "print(\"Texto limpio para spaCy:\", texto_limpio_spacy)"
      ],
      "metadata": {
        "id": "D4lS9v95ndWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e395f41-17f5-4593-880e-32587d24d9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto limpio para spaCy: los niños corrían rápidamente por el parque jugando y riendo qué día más lindo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Procesar el texto limpio con spaCy\n",
        "doc = nlp(texto_limpio_spacy)"
      ],
      "metadata": {
        "id": "448foMWQnitz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener los lemas, filtrando stopwords y tokens no alfabéticos\n",
        "lemas_spacy = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "\n",
        "print(\"\\nLemas resultantes (spaCy, filtrando stopwords y no alfabéticos):\", lemas_spacy)"
      ],
      "metadata": {
        "id": "fz4UMjZJnkaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0c0a07-faaf-4585-d205-5d8452dc54fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemas resultantes (spaCy, filtrando stopwords y no alfabéticos): ['niño', 'correr', 'rápidamente', 'parque', 'jugar', 'reir', 'lindo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos los lemas de las palabras relacionadas\n",
        "doc_relacionadas = nlp(\"correr corría corriendo corredor corredores\")\n",
        "lemas_relacionadas_spacy = [token.lemma_ for token in doc_relacionadas]\n",
        "print(f\"\\nLemas para {' '.join([t.text for t in doc_relacionadas])}: {lemas_relacionadas_spacy}\") # ¡Excelente! \"corredor\" es distinto de \"correr\""
      ],
      "metadata": {
        "id": "K_COplXgnugY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a279854f-3626-4427-ed58-3209ae1cbd27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemas para correr corría corriendo corredor corredores: ['correr', 'correr', 'correr', 'corredor', 'corredor']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_problematicas = nlp(\"computadora computación computar\")\n",
        "lemas_problematicas_spacy = [token.lemma_ for token in doc_problematicas]\n",
        "print(f\"Lemas para {' '.join([t.text for t in doc_problematicas])}: {lemas_problematicas_spacy}\") # \"computación\" se mantiene"
      ],
      "metadata": {
        "id": "yC98_51nnJhi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81987177-7d2f-425c-bcf4-80dea2eacfb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemas para computadora computación computar: ['computadoro', 'computación', 'computar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_problematicas_2 = nlp(\"universidad universo\")\n",
        "lemas_problematicas_2_spacy = [token.lemma_ for token in doc_problematicas_2]\n",
        "print(f\"Lemas para {' '.join([t.text for t in doc_problematicas_2])}: {lemas_problematicas_2_spacy}\") # Correctamente separados"
      ],
      "metadata": {
        "id": "v1NJHViin78w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a7df50c-12cf-46de-8a27-96bcaca05306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemas para universidad universo: ['universidad', 'universo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Comparación Stemming vs. Lematización\n",
        "\n",
        "Veamos lado a lado los resultados para nuestro texto de ejemplo:\n",
        "\n",
        "*   **Tokens originales (limpios):** `['niños', 'corrían', 'rápidamente', 'parque', 'jugando', 'riendo', 'día', 'lindo']`\n",
        "*   **Stems (NLTK):** `['niñ', 'corr', 'rapid', 'parqu', 'jug', 'riend', 'dia', 'lind']`\n",
        "*   **Lemas (spaCy):** `['niño', 'correr', 'rápidamente', 'parque', 'jugar', 'reír', 'día', 'lindo']`\n",
        "\n",
        "**Observaciones:**\n",
        "*   Los lemas son palabras reales, los stems no siempre.\n",
        "*   La lematización parece capturar mejor la forma base (\"correr\", \"jugar\", \"reír\").\n",
        "*   El stemming es más agresivo (\"niñ\", \"corr\", \"rapid\").\n",
        "*   Ambos acortan \"día\" (sin tilde) de forma similar en este caso (NLTK por stem, spaCy porque el modelo puede no tener la tilde en su lema base).\n",
        "\n",
        "**¿Cuándo usar cuál?**\n",
        "*   **Stemming:** Cuando la velocidad es crucial y no importa tanto la interpretabilidad (ej: recuperación de información a gran escala).\n",
        "*   **Lematización:** Cuando la precisión semántica y la interpretabilidad son importantes (ej: análisis de sentimiento, clasificación de temas, chatbots). **Generalmente preferido si los recursos computacionales lo permiten.**"
      ],
      "metadata": {
        "id": "7Py5ywppoQ-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Micro-Laboratorio (Ejercicio Práctico)\n",
        "\n",
        "**Consigna:**\n",
        "\n",
        "Dado el siguiente conjunto de frases (reviews de películas):\n",
        "1.  Definir una función `preprocesar_nltk(texto)` que:\n",
        "    *   Limpie el texto (minúsculas, números, puntuación).\n",
        "    *   Tokenice.\n",
        "    *   Quite stopwords (usando la lista de NLTK).\n",
        "    *   Aplique Stemming (con `SnowballStemmer`).\n",
        "    *   Devuelva la lista de stems.\n",
        "2.  Definir una función `preprocesar_spacy(texto)` que:\n",
        "    *   Limpie el texto (minúsculas, números, puntuación - cuidado con no quitar espacios necesarios para spaCy).\n",
        "    *   Procese el texto con `nlp()`.\n",
        "    *   Devuelva la lista de lemas de los tokens que no sean stopwords (`token.is_stop`) y sean alfabéticos (`token.is_alpha`).\n",
        "3.  Aplicar ambas funciones a cada frase del dataset `reviews`.\n",
        "4.  Imprimir los resultados de ambas funciones para cada frase, uno debajo del otro, para poder comparar.\n",
        "5.  **Observar:** ¿Qué diferencias notables encuentran? ¿En qué casos un método parece funcionar \"mejor\" que el otro?"
      ],
      "metadata": {
        "id": "VMhtHWA5oovZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset para el ejercicio\n",
        "reviews = [\n",
        "    \"Una película emocionante con actuaciones brillantes. ¡Me encantó!\",\n",
        "    \"Muy aburrida y lenta. El guión era predecible y los actores no convencían.\",\n",
        "    \"Los efectos especiales fueron impresionantes, pero la historia dejaba mucho que desear.\",\n",
        "    \"¡Qué gran comedia! Me reí sin parar durante toda la película.\",\n",
        "    \"Un documental necesario que aborda temas importantes con profundidad y sensibilidad.\"\n",
        "]"
      ],
      "metadata": {
        "id": "57JZ3yh6oSDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Definimos una funcion que limpie el texto"
      ],
      "metadata": {
        "id": "8abBcJOxpqE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos una función para limpiar el texto (minúsculas, números, puntuación). Limpie el texto (minúsculas, números, puntuación). Tokenice. Quite stopwords (usando la lista de NLTK). Aplique Stemming (con SnowballStemmer). Devuelva la lista de stems.\n",
        "def limpiar_tokenizar_basico(texto):\n",
        "  # 1. Minúsculas\n",
        "  texto = texto.lower()\n",
        "  # 2. Quitar números (usando expresiones regulares)\n",
        "  texto = re.sub(r'\\d+', '', texto)\n",
        "  # 3. Quitar puntuación\n",
        "  texto = texto.translate(str.maketrans('', '', string.punctuation + '¡¿'))\n",
        "  # 4. Quitar espacios extra\n",
        "  texto = texto.strip()\n",
        "  # 5. Tokenizar\n",
        "  tokens = nltk.word_tokenize(texto, language='spanish')\n",
        "  # 6. Quitar stopwords\n",
        "  tokens_limpios_2 = [palabra for palabra in tokens if palabra not in stopwords_es]\n",
        "  # 7. Aplique Stemming (con SnowballStemmer)\n",
        "  stems_nltk_2 = [stemmer.stem(token) for token in tokens_limpios_2]\n",
        "  return tokens_limpios_2, stems_nltk_2"
      ],
      "metadata": {
        "id": "zm2Y3wHdpoHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Definir una función que devuelva la lista de lemas de los tokens que no sean stopwords y sean alfabéticos\n"
      ],
      "metadata": {
        "id": "vKwPVpCMu5ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir una función preprocesar_spacy(texto) que: Limpie el texto (minúsculas, números, puntuación - cuidado con no quitar espacios necesarios para spaCy). Procese el texto con nlp(). Devuelva la lista de lemas de los tokens que no sean stopwords (token.is_stop) y sean alfabéticos (token.is_alpha).\n",
        "def preprocesar_spacy(texto):\n",
        "  # 1. Minúsculas\n",
        "  texto = texto.lower()\n",
        "  # 2. Numeros\n",
        "  texto = re.sub(r'\\d+', '', texto)\n",
        "  # 3. Quitar puntuación\n",
        "  texto = texto.translate(str.maketrans('', '', string.punctuation + '¡¿'))\n",
        "  # 4. Procese el texto con nlp()\n",
        "  doc = nlp(texto)\n",
        "  # 5. Devuelva la lista de lemas de los tokens que no sean stopwords (token.is_stop) y sean alfabéticos (token.is_alpha).\n",
        "  lemas_spacy = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "  return lemas_spacy"
      ],
      "metadata": {
        "id": "W831HRtStIf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Aplicamos las funciones en las reviews"
      ],
      "metadata": {
        "id": "yiLDTMHfvPU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar ambas funciones a cada frase del dataset reviews.\n",
        "reviews_stems = [limpiar_tokenizar_basico(review) for review in reviews]\n",
        "reviews_lema = [preprocesar_spacy(review) for review in reviews]"
      ],
      "metadata": {
        "id": "1cKD-2hHtxFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Imprimimos los resultados de ambas funciones para cada frase para poder comparar"
      ],
      "metadata": {
        "id": "KbDYM3hTvbO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(reviews_stems)\n",
        "print(reviews_lema)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ait-dIsPvmQj",
        "outputId": "fd0bfc68-cf2e-4103-a1de-f3a08de60f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['película', 'emocionante', 'actuaciones', 'brillantes', 'encantó'], ['aburrida', 'lenta', 'guión', 'predecible', 'actores', 'convencían'], ['efectos', 'especiales', 'impresionantes', 'historia', 'dejaba', 'desear'], ['gran', 'comedia', 'reí', 'parar', 'toda', 'película'], ['documental', 'necesario', 'aborda', 'temas', 'importantes', 'profundidad', 'sensibilidad']]\n",
            "[['película', 'emocionante', 'actuación', 'brillante', 'encantar'], ['aburrido', 'lento', 'guión', 'predecible', 'actor', 'convencer'], ['efecto', 'especial', 'impresionante', 'historia', 'dejar', 'desear'], ['comedia', 'reí', 'parar', 'película'], ['documental', 'necesario', 'abordar', 'tema', 'importante', 'profundidad', 'sensibilidad']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Brainstorming\n",
        "\n",
        "Ahora que conocemos estas técnicas, pensemos:\n",
        "\n",
        "**¿Cómo podemos preprocesar el texto de manera que se eviten sesgos y discriminaciones?**\n",
        "\n",
        "Priorizar la diversidad en el corpus de entrenamiento, asegurándose de que incluya representaciones equitativas de diferentes grupos demográficos, dialectos, registros lingüísticos y contextos culturales.\n",
        "\n",
        "Documentar cada paso del preprocesamiento, explicando las decisiones tomadas y por qué. Esto permite una revisión y auditoría posterior.\n",
        "\n",
        "*   ¿Qué pasa si la lista de `stopwords` que usamos (sea de NLTK o spaCy) quita palabras importantes para un grupo minoritario o en un contexto específico (ej: jerga, términos culturales)?\n",
        "*   ¿Los stemmers o lematizadores funcionan igual de bien con diferentes dialectos del español o con lenguaje inclusivo? (Probablemente no, los modelos estándar están entrenados en textos más \"formales\").\n",
        "*   Si quitamos nombres propios o entidades, ¿podríamos estar eliminando información crucial sobre representación?\n",
        "*   Al elegir agresividad (stemming) vs. precisión (lematización), ¿podríamos afectar diferencialmente el análisis de textos de distintos grupos?\n",
        "*   ¿Qué responsabilidad tenemos al elegir y aplicar estas técnicas? ¿Deberíamos documentar siempre nuestras decisiones de preprocesamiento?\n",
        "\n",
        "\n",
        "Las listas de stopwords estándar se construyen generalmente a partir de grandes corpus de texto \"general\" y están diseñadas para eliminar palabras de alta frecuencia que se consideran de poco valor semántico en un contexto amplio (ej. \"el\", \"la\", \"y\", \"es\"). Sin embargo existe la posibilidad de que se pierda información Contextual para un grupo minoritario o en un dominio específico, aumenta el Sesgo si las stopwords se eliminan de manera desigual entre diferentes grupos de texto. Subrepresentación: Si los términos culturales o específicos de la jerga de un grupo son eliminados, la capacidad del modelo para reconocer y representar a ese grupo se ve mermada.\n",
        "Esto se puede solucionar personalizando las listas de stopwords para no eliminarlas si la densidad de información en ellas es alta para el problema.\n"
      ],
      "metadata": {
        "id": "Av-E8AxOpFUP"
      }
    }
  ]
}