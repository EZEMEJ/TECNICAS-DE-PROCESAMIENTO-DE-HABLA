{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mzBx4CUBZLe1",
        "Mx_hzi_2ZeLf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TRABAJO PRÁCTICO FINAL INTEGRADOR\n",
        "\n",
        "Este proyecto es un notebook de Google Colab que permite realizar web scraping de artículos de noticias de Infobae y La Nación, unificarlos en un archivo CSV y aplicar diversas técnicas de procesamiento de lenguaje natural (NLP) y análisis de datos. Finalmente, se crea una interfaz interactiva utilizando Gradio para visualizar y analizar el contenido extraído y procesado.\n",
        "\n",
        "## Características:\n",
        "\n",
        "- **Web Scraping:** Extrae títulos y texto de los últimos artículos de Infobae y La Nación.\n",
        "- **Unificación de Datos:** Combina los datos de ambos sitios en un único DataFrame y lo exporta a un archivo CSV.\n",
        "- **Procesamiento de Lenguaje Natural (NLP):**\n",
        "    - **Limpieza y Tokenización:** Preprocesamiento del texto para eliminar ruido y dividirlo en unidades significativas.\n",
        "    - **Generación de Nubes de Palabras (WordCloud):** Visualización de las palabras más frecuentes en el texto.\n",
        "    - **Extracción de Entidades Nombradas (NER):** Identificación y clasificación de personas, lugares y organizaciones mencionadas en los artículos utilizando Gemini.\n",
        "    - **Análisis de Sentimiento:** Determinación de la polaridad (positivo, negativo, neutro) del texto utilizando TextBlob y visualización de la distribución general de sentimientos.\n",
        "    - **Generación de Resúmenes y Tweets:** Creación de resúmenes concisos y tweets con modismos argentinos utilizando Gemini.\n",
        "- **Interfaz Interactiva con Gradio:** Permite seleccionar artículos, visualizar su texto, aplicar funciones de NLP y ver los resultados de forma interactiva.\n",
        "\n",
        "## Cómo usar:\n",
        "\n",
        "1. **Abrir el Notebook:** Abre el notebook en Google Colab.\n",
        "2. **Ejecutar las Celdas:** Ejecuta secuencialmente todas las celdas del notebook. Esto instalará las dependencias, realizará el web scraping, procesará los datos y lanzará la interfaz Gradio.\n",
        "3. **Interactuar con la Interfaz Gradio:** Una vez que la interfaz Gradio se haya lanzado (aparecerá un enlace público), ábrela en tu navegador.\n",
        "    - Selecciona un artículo del menú desplegable.\n",
        "    - Utiliza los botones para:\n",
        "        - \"Mostrar texto\": Ver el texto completo del artículo.\n",
        "        - \"Limpiar texto\": Ver el texto después de la limpieza y tokenización.\n",
        "        - \"Generar WordCloud\": Visualizar la nube de palabras.\n",
        "        - \"Extraer Entidades (NER)\": Ver las entidades nombradas extraídas.\n",
        "        - \"Análisis de Sentimiento\": Ver el sentimiento del conjunto de artículos y la distribución general de sentimientos.\n",
        "        - \"Resumen\": Obtener un resumen del artículo generado por Gemini.\n",
        "        - \"Generador de Tweet\": Obtener un tweet generado por Gemini basado en el artículo.\n",
        "\n",
        "## Requisitos:\n",
        "\n",
        "- Cuenta de Google para acceder a Google Colab.\n",
        "- Clave de API de Google Gemini (configurada en los secretos de Colab como `GOOGLE_API_KEY`).\n",
        "\n",
        "## Dependencias:\n",
        "\n",
        "Las dependencias se instalan automáticamente al ejecutar la primera celda del notebook (`!pip install...`). Incluyen:\n",
        "\n",
        "- `gradio`\n",
        "- `transformers`\n",
        "- `sentencepiece`\n",
        "- `spacy`\n",
        "- `wordcloud`\n",
        "- `matplotlib`\n",
        "- `textblob`\n",
        "- `requests`\n",
        "- `beautifulsoup4`\n",
        "- `pandas`\n",
        "- `google-generativeai`\n",
        "- `nltk`\n",
        "\n",
        "## Archivos generados:\n",
        "\n",
        "- `datos_combinados.csv`: Archivo CSV que contiene los artículos scrapeados de Infobae y La Nación."
      ],
      "metadata": {
        "id": "7T4QK3LoZEJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Instalación de librerías y módulos necesarios."
      ],
      "metadata": {
        "id": "mzBx4CUBZLe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gradio transformers sentencepiece spacy wordcloud matplotlib textblob\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "JMOzAW_ScWRV",
        "collapsed": true
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from random import uniform\n",
        "from google.colab import userdata\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import spacy\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from textblob import TextBlob\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead, pipeline\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "DMXn6_uQTUYf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Web Scraping de Infobae y La Nación"
      ],
      "metadata": {
        "id": "Mx_hzi_2ZeLf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dhc2SYVE8wqt"
      },
      "outputs": [],
      "source": [
        "## WEB SCRAPING INFOBAE ##\n",
        "\n",
        "# URL base\n",
        "BASE_URL = \"https://www.infobae.com\"\n",
        "\n",
        "# URL que contiene los links de artículos (puedes cambiar esta a otra categoría o portada)\n",
        "page_url = \"https://www.infobae.com/ultimas-noticias/\"\n",
        "\n",
        "# Obtener el HTML\n",
        "response = requests.get(page_url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Extraer todos los <a> con clase \"story-card-ctn\"\n",
        "links = soup.find_all(\"a\", class_=\"feed-list-card\")\n",
        "\n",
        "# Extraer href y guardar en lista\n",
        "hrefs = [link.get(\"href\") for link in links if link.get(\"href\")]\n",
        "\n",
        "# Limitar a los primeros 20\n",
        "hrefs = hrefs[:20]\n",
        "\n",
        "# Para almacenar los resultados\n",
        "data = []\n",
        "\n",
        "# Recorrer cada href\n",
        "for href in hrefs:\n",
        "    full_url = BASE_URL + href if href.startswith(\"/\") else href\n",
        "    try:\n",
        "        # Pequeña pausa para no sobrecargar el servidor\n",
        "        sleep(uniform(1, 3))\n",
        "\n",
        "        article_response = requests.get(full_url)\n",
        "        article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
        "\n",
        "        # Extraer el título del artículo\n",
        "        h1_element = article_soup.find('h1')\n",
        "        id_value = h1_element.get('id', '')\n",
        "        titulo = id_value.replace('-', ' ')\n",
        "\n",
        "        # Extraer todos los <p> y juntar su texto\n",
        "        paragraphs = article_soup.find_all(\"p\")\n",
        "        texto = \" \".join([p.get_text(strip=False) for p in paragraphs])\n",
        "\n",
        "        # Guardar en data\n",
        "        data.append([\"infobae\", titulo, texto])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {full_url}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Crear DataFrame\n",
        "df_infobae = pd.DataFrame(data, columns=[\"fuente\", \"titulo\", \"texto\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- WEB SCRAPING LA NACION ---\n",
        "\n",
        "# URL base\n",
        "BASE_URL = \"https://www.lanacion.com.ar\"\n",
        "page_url = \"https://www.lanacion.com.ar/ultimas-noticias/\"\n",
        "\n",
        "# Obtener HTML de la página principal\n",
        "response = requests.get(page_url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Extraer todos los <a> con href (sin clase específica)\n",
        "links = soup.select(\"h2 a[href]\")\n",
        "\n",
        "# Filtrar hrefs que parezcan links válidos de artículos (opcional: que contengan alguna palabra clave o estructura)\n",
        "hrefs = [link['href'] for link in links if link['href'].startswith('/')]\n",
        "\n",
        "# Evitar duplicados\n",
        "hrefs = list(set(hrefs))\n",
        "\n",
        "# Limitar a los primeros 20\n",
        "hrefs = hrefs[:20]\n",
        "\n",
        "# Lista para almacenar los datos\n",
        "data = []\n",
        "\n",
        "# Recorrer los hrefs y scrapear cada artículo\n",
        "for href in hrefs:\n",
        "    full_url = BASE_URL + href\n",
        "    try:\n",
        "        sleep(uniform(1, 3))  # Pausa para evitar bloqueo\n",
        "\n",
        "        article_response = requests.get(full_url)\n",
        "        article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
        "\n",
        "        # Extraer el título del artículo\n",
        "        h1_element = article_soup.find('h1', class_=\"com-title\")\n",
        "        titulo = \"\".join(h1_element.get_text(strip=False))\n",
        "\n",
        "        # Extraer todo el texto de los <p>\n",
        "        paragraphs = article_soup.find_all(\"p\", class_=\"com-paragraph\")\n",
        "        texto = \" \".join([p.get_text(strip=False) for p in paragraphs])\n",
        "\n",
        "        # Agregar a la lista de resultados\n",
        "        data.append([\"lanacion\", titulo, texto])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar {full_url}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Crear un DataFrame con los resultados\n",
        "df_lanacion = pd.DataFrame(data, columns=[\"fuente\", \"titulo\", \"texto\"])\n",
        "\n",
        "df_lanacion.to_csv(\"lanacion.csv\", index=False)"
      ],
      "metadata": {
        "id": "IQkqmL2fBILP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unificamos los df de infobae y la nacion en un unico df que exportamos\n",
        "\n",
        "\n",
        "df = pd.concat([df_infobae, df_lanacion], ignore_index=True)\n",
        "\n",
        "# Guardar en un CSV\n",
        "df.to_csv(\"datos_combinados.csv\", index=False)\n",
        "\n",
        "print(\"✅ CSV guardado como 'datos_combinados.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtAf2setFk36",
        "outputId": "770b54c4-db8a-4efd-fa0f-ce36529260de"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CSV guardado como 'datos_combinados.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Funciones"
      ],
      "metadata": {
        "id": "6JnlA2jXZo55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo de Gemini\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "cliente = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "MODEL_ID = \"gemini-2.5-flash-preview-05-20\"\n",
        "\n",
        "# Cargar modelo spaCy español\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "nltk.download('punkt')  # Para la tokenización de palabras\n",
        "nltk.download('stopwords')  # Para las palabras comunes\n",
        "\n",
        "# Cargar CSV\n",
        "df = pd.read_csv(\"datos_combinados.csv\")\n",
        "\n",
        "# Opciones para dropdown\n",
        "opciones = df[\"titulo\"].dropna().astype(str).unique().tolist()\n",
        "\n",
        "# Cargar el modelo de resumen de Hugging Face (T5 o BART para español)\n",
        "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "# summarizer = pipeline(\"summarization\", model=\"josmunpen/mt5-small-spanish-summarization\")\n",
        "# summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\")\n",
        "\n",
        "# Función para mostrar texto\n",
        "def mostrar_texto(href_seleccionado):\n",
        "    fila = df[df[\"titulo\"].astype(str) == str(href_seleccionado)]\n",
        "    if fila.empty:\n",
        "        return \"⚠️ No se encontró el artículo.\"\n",
        "    texto = fila.iloc[0][\"texto\"]\n",
        "    if not texto or texto.strip() == \"\":\n",
        "        return \"⚠️ Artículo sin contenido válido.\"\n",
        "    return texto\n",
        "\n",
        "# 1. Funciones de limpieza y wordcloud\n",
        "\n",
        "# Función de Limpieza\n",
        "def limpiar_texto(texto):\n",
        "    texto = texto[11:]\n",
        "    texto = re.sub(r\"[^\\w\\s]\", \"\", texto)  # Quitar puntuación\n",
        "    texto = re.sub(r\"\\d+\", \"\", texto)      # Quitar números\n",
        "    texto = texto.lower()       # Normalizar y pasar a minúsculas\n",
        "    return texto.strip()\n",
        "\n",
        "\n",
        "# Preprocesamiento NLP\n",
        "def procesar_texto(texto):\n",
        "    texto_limpio = limpiar_texto(texto)\n",
        "    doc = nlp(texto_limpio)\n",
        "    tokens = [t.lemma_ for t in doc if t.is_alpha and not t.is_stop]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "# Generar WordCloud\n",
        "def generar_wordcloud(texto):\n",
        "    if not texto or not isinstance(texto, str) or texto.strip() == \"\":\n",
        "        print(\"⚠️ Texto vacío o inválido.\")\n",
        "        return None\n",
        "    try:\n",
        "        doc = nlp(texto)\n",
        "        tokens = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop and len(token.text) > 2]\n",
        "        if not tokens:\n",
        "            print(\"⚠️ No hay tokens válidos después de filtrar.\")\n",
        "            return None\n",
        "        texto_procesado = \" \".join(tokens)\n",
        "        wordcloud = WordCloud(width=800, height=400, max_words=100, background_color=\"white\").generate(texto_procesado)\n",
        "        buf = BytesIO()\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.tight_layout(pad=0)\n",
        "        plt.savefig(buf, format='png')\n",
        "        plt.close()\n",
        "        buf.seek(0)\n",
        "        img = Image.open(buf)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error generando WordCloud: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# 2. Función para NER: extraer y contar entidades en texto seleccionado\n",
        "\n",
        "def realizar_ner(texto):\n",
        "    pregunta = f\"\"\"\n",
        "    Extraé todas las entidades nombradas del siguiente texto en español argentino y clasificalas, solo necesito la respuesta:\n",
        "\n",
        "    CATEGORÍAS:\n",
        "    - PERSONA: Nombres de personas\n",
        "    - LUGAR: Ciudades, países, barrios, direcciones, lugares específicos\n",
        "    - ORGANIZACIÓN: Empresas, universidades, instituciones\n",
        "\n",
        "    FORMATO DE RESPUESTA:\n",
        "    [ENTIDAD] → [CATEGORÍA] → [BREVE EXPLICACIÓN]\n",
        "\n",
        "    TEXTO A ANALIZAR:\n",
        "    {texto}\n",
        "    \"\"\"\n",
        "    respuesta = cliente.models.generate_content(\n",
        "                model=MODEL_ID,\n",
        "                contents=[pregunta] # Pasa la pregunta como contenido\n",
        "                    )\n",
        "    return respuesta.text\n",
        "\n",
        "\n",
        "\n",
        "# 3. Funciones para análisis de sentimiento\n",
        "\n",
        "# Función auxiliar para clasificar polaridad con TextBlob\n",
        "def clasificar_sentimiento(texto):\n",
        "    if not texto or texto.strip() == \"\":\n",
        "        return \"Neutro\"\n",
        "    try:\n",
        "        blob = TextBlob(texto)\n",
        "        pol = blob.sentiment.polarity\n",
        "        if pol > 0.1:\n",
        "            return \"Positivo\"\n",
        "        elif pol < -0.1:\n",
        "            return \"Negativo\"\n",
        "        else:\n",
        "            return \"Neutro\"\n",
        "    except:\n",
        "        return \"Neutro\"\n",
        "\n",
        "# Calcular sentimiento para todas las noticias del csv\n",
        "df['sentimiento'] = df['texto'].fillna(\"\").apply(clasificar_sentimiento)\n",
        "\n",
        "# Gráfico de barras con distribución general\n",
        "def graficar_distribucion_sentimientos():\n",
        "    conteo = df['sentimiento'].value_counts()\n",
        "    plt.figure(figsize=(6,4))\n",
        "    conteo.plot(kind='bar', color=['green','red','gray'])\n",
        "    plt.title(\"Distribución de sentimiento en artículos\")\n",
        "    plt.ylabel(\"Cantidad de artículos\")\n",
        "    plt.xlabel(\"Sentimiento\")\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close()\n",
        "    buf.seek(0)\n",
        "    return Image.open(buf)\n",
        "\n",
        "# Mostrar sentimiento y gráfica de distribución al seleccionar artículo\n",
        "def analizar_sentimiento(titulo_articulo):\n",
        "    fila = df[df[\"titulo\"].astype(str) == str(titulo_articulo)]\n",
        "    if fila.empty:\n",
        "        return \"No se encontró el artículo.\", None\n",
        "    texto = fila.iloc[0][\"texto\"]\n",
        "    sentimiento_articulo = clasificar_sentimiento(texto)  # Analiza solo este texto\n",
        "    grafico = graficar_distribucion_sentimientos()\n",
        "    texto_salida = f\"Sentimiento del artículo seleccionado: **{sentimiento_articulo}**\"\n",
        "    return texto_salida, grafico\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 4. Función de resumen y generación de tweet\n",
        "\n",
        "# Función para generar resumen\n",
        "def generar_resumen(texto):\n",
        "    pregunta = f\"\"\"Sumariza el siguiente texto en tres oraciones seguidas relacionadas de rapida lectura\n",
        "              Texto: {texto}\n",
        "              \"\"\"\n",
        "\n",
        "    resumen_final = cliente.models.generate_content(\n",
        "                    model=MODEL_ID,\n",
        "                    contents=[pregunta] # Pasa la pregunta como contenido\n",
        "                    )\n",
        "    return resumen_final.text\n",
        "\n",
        "\n",
        "# Función para generar tweet\n",
        "def generar_tweet(texto):\n",
        "    pregunta = f\"\"\"Sumariza en un tweet de 80 palabras máximo utilizando modismos argentinos, no es necesario que remarques los modismos\n",
        "              Texto: {texto}\n",
        "              \"\"\"\n",
        "\n",
        "    tweet_final = cliente.models.generate_content(\n",
        "                    model=MODEL_ID,\n",
        "                    contents=[pregunta] # Pasa la pregunta como contenido\n",
        "                    )\n",
        "    return tweet_final.text\n",
        "\n"
      ],
      "metadata": {
        "id": "YKelagR7SerF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f66ae7-2615-4cc3-9b0a-9f002af8359e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Interfaz Gradio"
      ],
      "metadata": {
        "id": "VAHeT_DDZ3CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- INTERFAZ GRADIO ---\n",
        "\n",
        "with gr.Blocks() as interfaz:\n",
        "    gr.Markdown(\"## 📰 Visualizador\")\n",
        "\n",
        "    dropdown = gr.Dropdown(choices=opciones, label=\"Seleccioná un artículo\")\n",
        "\n",
        "    with gr.Row():\n",
        "        boton_texto = gr.Button(\"Mostrar texto\")\n",
        "        boton_limpieza = gr.Button(\"Limpiar texto\")\n",
        "        boton_wc = gr.Button(\"Generar WordCloud\")\n",
        "        boton_ner = gr.Button(\"Extraer Entidades (NER)\")\n",
        "        boton_sent = gr.Button(\"Análisis de Sentimiento\")\n",
        "\n",
        "    with gr.Row():\n",
        "        boton_resumen = gr.Button(\"Resumen\")\n",
        "        boton_opinion = gr.Button(\"Generador de Tweet\")\n",
        "\n",
        "    salida = gr.Textbox(label=\"Texto del artículo\", lines=15)\n",
        "    salida_texto = gr.Textbox(label=\"Limpieza\", lines=15)\n",
        "    salida_imagen = gr.Image(type=\"pil\", label=\"WordCloud\")\n",
        "    salida_ner = gr.Textbox(label=\"Entidades nombradas extraídas\")\n",
        "    salida_sentimiento = gr.Markdown(label=\"Resultado de análisis de sentimiento\")\n",
        "    salida_grafico_sent = gr.Image(type=\"pil\", label=\"Distribución general sentimientos del conjunto de noticias\")\n",
        "    salida_resumen = gr.Textbox(label=\"Resumen de la noticia\", lines=3)\n",
        "    salida_opinion = gr.Textbox(label=\"Generador de Tweet\", lines=4)\n",
        "\n",
        "\n",
        "    # Eventos botones\n",
        "    boton_texto.click(fn=mostrar_texto, inputs=dropdown, outputs=salida)\n",
        "    boton_limpieza.click(fn=procesar_texto, inputs=salida, outputs=salida_texto)\n",
        "    boton_wc.click(fn=generar_wordcloud, inputs=salida_texto, outputs=salida_imagen)\n",
        "    boton_ner.click(fn=realizar_ner, inputs=salida_texto, outputs=salida_ner)\n",
        "    boton_sent.click(fn=analizar_sentimiento, inputs=dropdown, outputs=[salida_sentimiento, salida_grafico_sent])\n",
        "    boton_resumen.click(fn=generar_resumen, inputs=salida_texto, outputs=salida_resumen)\n",
        "    boton_opinion.click(fn=generar_tweet, inputs=salida_texto, outputs=salida_opinion)\n",
        "\n",
        "\n",
        "interfaz.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "MFmvsG0RXI5e",
        "outputId": "5b299138-c49f-4bd6-df13-5782748a5838"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://988e7dab409dbcefdd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://988e7dab409dbcefdd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}