# Modelos de Lenguaje Basados en Transformers

Este repositorio contiene notebooks enfocados en el estudio de los modelos de lenguaje modernos, en particular aquellos basados en la arquitectura **Transformer**, que ha revolucionado el procesamiento del lenguaje natural (PLN) en los 煤ltimos a帽os.

---

##  Notebooks incluidos

### 1. **Modelos_de_Lenguaje_Transformer.ipynb**

Este notebook introduce los fundamentos de los modelos de lenguaje con arquitectura Transformer, incluyendo:

- Arquitectura general de Transformer
- Mecanismo de atenci贸n (self-attention)
- Ventajas sobre modelos secuenciales cl谩sicos (RNN, LSTM)
- Aplicaciones en NLP modernas (traducci贸n autom谩tica, resumen, clasificaci贸n)

### 2. **Transformers.ipynb**

Complementa el anterior con una implementaci贸n pr谩ctica del uso de modelos preentrenados mediante bibliotecas como **Hugging Face Transformers**. Contenidos destacados:

- Tokenizaci贸n y embeddings con `AutoTokenizer`
- Carga y uso de modelos preentrenados (`AutoModel`, `pipeline`)
- Ejemplos de clasificaci贸n de texto y an谩lisis sem谩ntico
- Interpretaci贸n de salidas y visualizaci贸n de resultados

---

##  Requisitos sugeridos

- Python 3.8+
- Bibliotecas:
  - `transformers`
  - `torch`
  - `numpy`
  - `pandas`

> Se recomienda ejecutar estos notebooks en un entorno como Google Colab si no se dispone de GPU local.

---

##  Objetivo

Brindar una base te贸rica y pr谩ctica s贸lida para comprender y aplicar modelos de lenguaje modernos en tareas reales de PLN.

