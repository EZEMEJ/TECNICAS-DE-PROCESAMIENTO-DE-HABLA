# Modelos de Lenguaje Basados en Transformers

Este repositorio contiene notebooks enfocados en el estudio de los modelos de lenguaje modernos, en particular aquellos basados en la arquitectura **Transformer**, que ha revolucionado el procesamiento del lenguaje natural (PLN) en los últimos años.

---

## 📘 Notebooks incluidos

### 1. **Modelos_de_Lenguaje_Transformer.ipynb**

Este notebook introduce los fundamentos de los modelos de lenguaje con arquitectura Transformer, incluyendo:

- Arquitectura general de Transformer
- Mecanismo de atención (self-attention)
- Ventajas sobre modelos secuenciales clásicos (RNN, LSTM)
- Aplicaciones en NLP modernas (traducción automática, resumen, clasificación)

### 2. **Transformers.ipynb**

Complementa el anterior con una implementación práctica del uso de modelos preentrenados mediante bibliotecas como **Hugging Face Transformers**. Contenidos destacados:

- Tokenización y embeddings con `AutoTokenizer`
- Carga y uso de modelos preentrenados (`AutoModel`, `pipeline`)
- Ejemplos de clasificación de texto y análisis semántico
- Interpretación de salidas y visualización de resultados

---

## 🔧 Requisitos sugeridos

- Python 3.8+
- Bibliotecas:
  - `transformers`
  - `torch`
  - `numpy`
  - `pandas`

> Se recomienda ejecutar estos notebooks en un entorno como Google Colab si no se dispone de GPU local.

---

## 📚 Objetivo

Brindar una base teórica y práctica sólida para comprender y aplicar modelos de lenguaje modernos en tareas reales de PLN.

